<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>adapt.parameter_based.FineTuning &mdash; adapt 0.1.0 documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/sg_gallery.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script src="../_static/js/custom.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> adapt
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu"> 
<p class="caption" role="heading"><span class="caption-text">Installation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://github.com/adapt-python/adapt">Github</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../map.html">Choosing the right algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples/Quick_start.html">Quick-Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples/Developer_Guide.html">Developer Guide</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Documentation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../contents.html#adapt-feature-based">Feature-based</a><ul>
<li class="toctree-l2"><a class="reference internal" href="adapt.feature_based.PRED.html">PRED</a></li>
<li class="toctree-l2"><a class="reference internal" href="adapt.feature_based.FA.html">FA</a></li>
<li class="toctree-l2"><a class="reference internal" href="adapt.feature_based.CORAL.html">CORAL</a></li>
<li class="toctree-l2"><a class="reference internal" href="adapt.feature_based.SA.html">SA</a></li>
<li class="toctree-l2"><a class="reference internal" href="adapt.feature_based.TCA.html">TCA</a></li>
<li class="toctree-l2"><a class="reference internal" href="adapt.feature_based.fMMD.html">fMMD</a></li>
<li class="toctree-l2"><a class="reference internal" href="adapt.feature_based.DeepCORAL.html">DeepCORAL</a></li>
<li class="toctree-l2"><a class="reference internal" href="adapt.feature_based.DANN.html">DANN</a></li>
<li class="toctree-l2"><a class="reference internal" href="adapt.feature_based.ADDA.html">ADDA</a></li>
<li class="toctree-l2"><a class="reference internal" href="adapt.feature_based.WDGRL.html">WDGRL</a></li>
<li class="toctree-l2"><a class="reference internal" href="adapt.feature_based.CDAN.html">CDAN</a></li>
<li class="toctree-l2"><a class="reference internal" href="adapt.feature_based.MCD.html">MCD</a></li>
<li class="toctree-l2"><a class="reference internal" href="adapt.feature_based.MDD.html">MDD</a></li>
<li class="toctree-l2"><a class="reference internal" href="adapt.feature_based.CCSA.html">CCSA</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../contents.html#adapt-instance-based">Instance-based</a><ul>
<li class="toctree-l2"><a class="reference internal" href="adapt.instance_based.LDM.html">LDM</a></li>
<li class="toctree-l2"><a class="reference internal" href="adapt.instance_based.KLIEP.html">KLIEP</a></li>
<li class="toctree-l2"><a class="reference internal" href="adapt.instance_based.KMM.html">KMM</a></li>
<li class="toctree-l2"><a class="reference internal" href="adapt.instance_based.ULSIF.html">ULSIF</a></li>
<li class="toctree-l2"><a class="reference internal" href="adapt.instance_based.RULSIF.html">RULSIF</a></li>
<li class="toctree-l2"><a class="reference internal" href="adapt.instance_based.NearestNeighborsWeighting.html">NearestNeighborsWeighting</a></li>
<li class="toctree-l2"><a class="reference internal" href="adapt.instance_based.IWC.html">IWC</a></li>
<li class="toctree-l2"><a class="reference internal" href="adapt.instance_based.IWN.html">IWN</a></li>
<li class="toctree-l2"><a class="reference internal" href="adapt.instance_based.BalancedWeighting.html">BalancedWeighting</a></li>
<li class="toctree-l2"><a class="reference internal" href="adapt.instance_based.TrAdaBoost.html">TrAdaBoost</a></li>
<li class="toctree-l2"><a class="reference internal" href="adapt.instance_based.TrAdaBoostR2.html">TrAdaBoostR2</a></li>
<li class="toctree-l2"><a class="reference internal" href="adapt.instance_based.TwoStageTrAdaBoostR2.html">TwoStageTrAdaBoostR2</a></li>
<li class="toctree-l2"><a class="reference internal" href="adapt.instance_based.WANN.html">WANN</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../contents.html#adapt-parameter-based">Parameter-based</a><ul>
<li class="toctree-l2"><a class="reference internal" href="adapt.parameter_based.LinInt.html">LinInt</a></li>
<li class="toctree-l2"><a class="reference internal" href="adapt.parameter_based.RegularTransferLR.html">RegularTransferLR</a></li>
<li class="toctree-l2"><a class="reference internal" href="adapt.parameter_based.RegularTransferLC.html">RegularTransferLC</a></li>
<li class="toctree-l2"><a class="reference internal" href="adapt.parameter_based.RegularTransferNN.html">RegularTransferNN</a></li>
<li class="toctree-l2"><a class="reference internal" href="adapt.parameter_based.RegularTransferGP.html">RegularTransferGP</a></li>
<li class="toctree-l2"><a class="reference internal" href="#">FineTuning</a></li>
<li class="toctree-l2"><a class="reference internal" href="adapt.parameter_based.TransferTreeClassifier.html">TransferTreeClassifier</a></li>
<li class="toctree-l2"><a class="reference internal" href="adapt.parameter_based.TransferForestClassifier.html">TransferForestClassifier</a></li>
<li class="toctree-l2"><a class="reference internal" href="adapt.parameter_based.TransferTreeSelector.html">TransferTreeSelector</a></li>
<li class="toctree-l2"><a class="reference internal" href="adapt.parameter_based.TransferForestSelector.html">TransferForestSelector</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../contents.html#adapt-metrics">Metrics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="adapt.metrics.make_uda_scorer.html">make_uda_scorer</a></li>
<li class="toctree-l2"><a class="reference internal" href="adapt.metrics.cov_distance.html">cov_distance</a></li>
<li class="toctree-l2"><a class="reference internal" href="adapt.metrics.neg_j_score.html">neg_j_score</a></li>
<li class="toctree-l2"><a class="reference internal" href="adapt.metrics.linear_discrepancy.html">linear_discrepancy</a></li>
<li class="toctree-l2"><a class="reference internal" href="adapt.metrics.normalized_linear_discrepancy.html">normalized_linear_discrepancy</a></li>
<li class="toctree-l2"><a class="reference internal" href="adapt.metrics.frechet_distance.html">frechet_distance</a></li>
<li class="toctree-l2"><a class="reference internal" href="adapt.metrics.normalized_frechet_distance.html">normalized_frechet_distance</a></li>
<li class="toctree-l2"><a class="reference internal" href="adapt.metrics.domain_classifier.html">domain_classifier</a></li>
<li class="toctree-l2"><a class="reference internal" href="adapt.metrics.reverse_validation.html">reverse_validation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../contents.html#adapt-utils">Utility Functions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="adapt.utils.UpdateLambda.html">UpdateLambda</a></li>
<li class="toctree-l2"><a class="reference internal" href="adapt.utils.accuracy.html">accuracy</a></li>
<li class="toctree-l2"><a class="reference internal" href="adapt.utils.check_arrays.html">check_arrays</a></li>
<li class="toctree-l2"><a class="reference internal" href="adapt.utils.check_estimator.html">check_estimator</a></li>
<li class="toctree-l2"><a class="reference internal" href="adapt.utils.check_network.html">check_network</a></li>
<li class="toctree-l2"><a class="reference internal" href="adapt.utils.get_default_encoder.html">get_default_encoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="adapt.utils.get_default_task.html">get_default_task</a></li>
<li class="toctree-l2"><a class="reference internal" href="adapt.utils.get_default_discriminator.html">get_default_discriminator</a></li>
<li class="toctree-l2"><a class="reference internal" href="adapt.utils.GradientHandler.html">GradientHandler</a></li>
<li class="toctree-l2"><a class="reference internal" href="adapt.utils.make_classification_da.html">make_classification_da</a></li>
<li class="toctree-l2"><a class="reference internal" href="adapt.utils.make_regression_da.html">make_regression_da</a></li>
<li class="toctree-l2"><a class="reference internal" href="adapt.utils.check_sample_weight.html">check_sample_weight</a></li>
<li class="toctree-l2"><a class="reference internal" href="adapt.utils.set_random_seed.html">set_random_seed</a></li>
<li class="toctree-l2"><a class="reference internal" href="adapt.utils.check_fitted_estimator.html">check_fitted_estimator</a></li>
<li class="toctree-l2"><a class="reference internal" href="adapt.utils.check_fitted_network.html">check_fitted_network</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Synthetic Examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../examples/Classification.html">Classification</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../examples/Classification.html#Experimental-Setup">Experimental Setup</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/Classification.html#Src-Only">Src Only</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/Classification.html#DANN">DANN</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/Classification.html#Instance-Based">Instance Based</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../examples/Two_moons.html">Two Moons</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../examples/Two_moons.html#Setup">Setup</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/Two_moons.html#Network">Network</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/Two_moons.html#Source-Only">Source Only</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/Two_moons.html#DANN">DANN</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/Two_moons.html#ADDA">ADDA</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/Two_moons.html#DeepCORAL">DeepCORAL</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/Two_moons.html#CORAL">CORAL</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/Two_moons.html#MCD">MCD</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/Two_moons.html#MDD">MDD</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/Two_moons.html#WDGRL">WDGRL</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/Two_moons.html#CDAN">CDAN</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../examples/Rotation.html">Rotation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../examples/Rotation.html#Setup">Setup</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/Rotation.html#Source-Only">Source Only</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/Rotation.html#CORAL">CORAL</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/Rotation.html#DANN">DANN</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/Rotation.html#ADDA">ADDA</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../examples/Regression.html">Toy Regression</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../examples/Regression.html#Experimental-Setup">Experimental Setup</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/Regression.html#TGT-Only">TGT Only</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/Regression.html#Src-Only">Src Only</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/Regression.html#All">All</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/Regression.html#CORAL">CORAL</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/Regression.html#TrAdaBoostR2">TrAdaBoostR2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/Regression.html#RegularTransferNN">RegularTransferNN</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../examples/sample_bias.html">Sample Bias 1D</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../examples/sample_bias.html#Setup">Setup</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/sample_bias.html#KMM">KMM</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/sample_bias.html#KLIEP">KLIEP</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../examples/sample_bias_2d.html">Sample Bias 2D</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../examples/sample_bias_2d.html#Setup">Setup</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/sample_bias_2d.html#Estimator">Estimator</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/sample_bias_2d.html#Source-Only">Source Only</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/sample_bias_2d.html#KMM">KMM</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/sample_bias_2d.html#KLIEP">KLIEP</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../examples/Multi_fidelity.html">Multi-Fidelity</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../examples/Multi_fidelity.html#Setup">Setup</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/Multi_fidelity.html#Network">Network</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/Multi_fidelity.html#Low-fidelity-only">Low fidelity only</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/Multi_fidelity.html#High-fidelity-only">High fidelity only</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/Multi_fidelity.html#RegularTransferNN">RegularTransferNN</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Real Examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../examples/Sample_bias_example.html">Sample Bias</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../examples/Sample_bias_example.html#Sample-Bias-on-the-diabetes-dataset">Diabetes Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/Sample_bias_example.html#Applying-Transfer-Learning-for-correcting-sample-bias">Sample bias correction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/Sample_bias_example.html#Hidden-bias-and-features-importance-estimation">Features importance</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../examples/Flowers_example.html">Fine-Tuning</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../examples/Flowers_example.html#Training-a-model-from-scratch">Train from Scratch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/Flowers_example.html#Model-based-Transfer">Model-based Transfer</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../examples/Office_example.html">Deep Domain Adaptation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../examples/Office_example.html#Dataset-Preprocessing">Dataset Preprocessing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/Office_example.html#Fit-without-adaptation">Fit without adaptation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/Office_example.html#Fit-with-adaptation">Fit with adaptation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../examples/tradaboost_experiments.html">TrAdaBoost Experiments</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../examples/tradaboost_experiments.html#Mushrooms">Mushrooms</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/tradaboost_experiments.html#20-NewsGroup">20-NewsGroup</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../examples/Heart_Failure.html">Heart Failure</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../examples/Heart_Failure.html#Setup">Setup</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/Heart_Failure.html#Network">Network</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/Heart_Failure.html#Source-Only">Source Only</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/Heart_Failure.html#DANN">DANN</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/Heart_Failure.html#ADDA">ADDA</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/Heart_Failure.html#DeepCORAL">DeepCORAL</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/Heart_Failure.html#CORAL">CORAL</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/Heart_Failure.html#MCD">MCD</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/Heart_Failure.html#MDD">MDD</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/Heart_Failure.html#WDGRL">WDGRL</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/Heart_Failure.html#CDAN">CDAN</a></li>
</ul>
</li>
</ul>
 
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">adapt</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li><span class="xref std std-ref">adapt.parameter_based</span>.FineTuning</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="adapt-parameter-based-finetuning">
<h1><a class="reference internal" href="../contents.html#adapt-parameter-based"><span class="std std-ref">adapt.parameter_based</span></a>.FineTuning<a class="headerlink" href="#adapt-parameter-based-finetuning" title="Permalink to this headline"></a></h1>
<dl class="py class">
<dt class="sig sig-object py" id="adapt.parameter_based.FineTuning">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">adapt.parameter_based.</span></span><span class="sig-name descname"><span class="pre">FineTuning</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">encoder</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">task</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Xt</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">yt</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pretrain</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">copy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">params</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/antoinedemathelin/adapt/tree/master/adapt/parameter_based/_finetuning.py"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#adapt.parameter_based.FineTuning" title="Permalink to this definition"></a></dt>
<dd><p>FineTuning : finetunes pretrained networks on target data.</p>
<p>A pretrained source encoder should be given. A task network,
pretrained or not, should be given too.</p>
<p>Finetuning train both networks. The task network can be
fitted first using the <code class="docutils literal notranslate"><span class="pre">pretrain</span></code> parameter. The layers
to train in the encoder can be set via the parameter <code class="docutils literal notranslate"><span class="pre">training</span></code>.</p>
<figure class="align-center" id="id3">
<img alt="../_images/regulartransfer.png" src="../_images/regulartransfer.png" />
<figcaption>
<p><span class="caption-text">Transferring parameters of a CNN pretrained on Imagenet
(source: [1])</span><a class="headerlink" href="#id3" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>encoder</strong><span class="classifier">tensorflow Model (default=None)</span></dt><dd><p>Encoder netwok. If <code class="docutils literal notranslate"><span class="pre">None</span></code>, a shallow network with 10
neurons and ReLU activation is used as encoder network.</p>
</dd>
<dt><strong>task</strong><span class="classifier">tensorflow Model (default=None)</span></dt><dd><p>Task netwok. If <code class="docutils literal notranslate"><span class="pre">None</span></code>, a two layers network with 10
neurons per layer and ReLU activation is used as task network.</p>
</dd>
<dt><strong>Xt</strong><span class="classifier">numpy array (default=None)</span></dt><dd><p>Target input data.</p>
</dd>
<dt><strong>yt</strong><span class="classifier">numpy array (default=None)</span></dt><dd><p>Target output data.</p>
</dd>
<dt><strong>training</strong><span class="classifier">bool or list of bool, optional (default=True)</span></dt><dd><p>Weither to train the <code class="docutils literal notranslate"><span class="pre">encoder</span></code> or not.
If a list is given, values from <code class="docutils literal notranslate"><span class="pre">training</span></code> are assigned
successively to the <code class="docutils literal notranslate"><span class="pre">trainable</span></code> attribute of the
<code class="docutils literal notranslate"><span class="pre">encoder</span></code> layers going from the last layer to the first one.
If the length of <code class="docutils literal notranslate"><span class="pre">training</span></code> is smaller than the length of
the <code class="docutils literal notranslate"><span class="pre">encoder</span></code> layers list, the last value of <code class="docutils literal notranslate"><span class="pre">training</span></code> 
will be asigned to the remaining layers.</p>
</dd>
<dt><strong>pretrain</strong><span class="classifier">bool (default=False)</span></dt><dd><p>If True, the task network is first trained alone on the outputs
of the encoder.</p>
</dd>
<dt><strong>copy</strong><span class="classifier">boolean (default=True)</span></dt><dd><p>Whether to make a copy of <code class="docutils literal notranslate"><span class="pre">estimator</span></code> or not.</p>
</dd>
<dt><strong>verbose</strong><span class="classifier">int (default=1)</span></dt><dd><p>Verbosity level.</p>
</dd>
<dt><strong>random_state</strong><span class="classifier">int (default=None)</span></dt><dd><p>Seed of random generator.</p>
</dd>
<dt><strong>params</strong><span class="classifier">key, value arguments</span></dt><dd><p>Arguments given at the different level of the adapt object.
It can be, for instance, compile or fit parameters of the
estimator or kernel parameters etc…
Accepted parameters can be found by calling the method
<code class="docutils literal notranslate"><span class="pre">_get_legal_params(params)</span></code>.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Yields</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>optimizer</strong><span class="classifier">str or instance of tf.keras.optimizers (default=”rmsprop”)</span></dt><dd><p>Optimizer for the task. It should be an
instance of tf.keras.optimizers as:
<code class="docutils literal notranslate"><span class="pre">tf.keras.optimizers.SGD(0.001)</span></code> or
<code class="docutils literal notranslate"><span class="pre">tf.keras.optimizers.Adam(lr=0.001,</span> <span class="pre">beta_1=0.5)</span></code>.
A string can also be given as <code class="docutils literal notranslate"><span class="pre">&quot;adam&quot;</span></code>.
Default optimizer is <code class="docutils literal notranslate"><span class="pre">rmsprop</span></code>.</p>
</dd>
<dt><strong>loss</strong><span class="classifier">str or instance of tf.keras.losses (default=”mse”)</span></dt><dd><p>Loss for the task. It should be an
instance of tf.keras.losses as:
<code class="docutils literal notranslate"><span class="pre">tf.keras.losses.MeanSquaredError()</span></code> or
<code class="docutils literal notranslate"><span class="pre">tf.keras.losses.CategoricalCrossentropy()</span></code>.
A string can also be given as <code class="docutils literal notranslate"><span class="pre">&quot;mse&quot;</span></code> or
<code class="docutils literal notranslate"><span class="pre">categorical_crossentropy</span></code>.
Default loss is <code class="docutils literal notranslate"><span class="pre">mse</span></code>.</p>
</dd>
<dt><strong>metrics</strong><span class="classifier">list of str or list of tf.keras.metrics.Metric instance</span></dt><dd><p>List of metrics to be evaluated by the model during training
and testing. Typically you will use <code class="docutils literal notranslate"><span class="pre">metrics=['accuracy']</span></code>.</p>
</dd>
<dt><strong>optimizer_enc</strong><span class="classifier">str or instance of tf.keras.optimizers</span></dt><dd><p>If the Adapt Model has an <code class="docutils literal notranslate"><span class="pre">encoder</span></code> attribute,
a specific optimizer for the <code class="docutils literal notranslate"><span class="pre">encoder</span></code> network can
be given. Typically, this parameter can be used to
give a smaller learning rate to the encoder.
If not specified, <code class="docutils literal notranslate"><span class="pre">optimizer_enc=optimizer</span></code>.</p>
</dd>
<dt><strong>optimizer_disc</strong><span class="classifier">str or instance of tf.keras.optimizers</span></dt><dd><p>If the Adapt Model has a <code class="docutils literal notranslate"><span class="pre">discriminator</span></code> attribute,
a specific optimizer for the <code class="docutils literal notranslate"><span class="pre">discriminator</span></code> network can
be given. If not specified, <code class="docutils literal notranslate"><span class="pre">optimizer_disc=optimizer</span></code>.</p>
</dd>
<dt><strong>kwargs</strong><span class="classifier">key, value arguments</span></dt><dd><p>Any arguments of the <code class="docutils literal notranslate"><span class="pre">fit</span></code> method from the Tensorflow
Model can be given, as <code class="docutils literal notranslate"><span class="pre">epochs</span></code> and <code class="docutils literal notranslate"><span class="pre">batch_size</span></code>.
Specific arguments from <code class="docutils literal notranslate"><span class="pre">optimizer</span></code> can also be given
as <code class="docutils literal notranslate"><span class="pre">learning_rate</span></code> or <code class="docutils literal notranslate"><span class="pre">beta_1</span></code> for <code class="docutils literal notranslate"><span class="pre">Adam</span></code>.
This allows to perform <code class="docutils literal notranslate"><span class="pre">GridSearchCV</span></code> from scikit-learn
on these arguments.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">References</p>
<dl class="citation">
<dt class="label" id="r8f631e03c5d8-1"><span class="brackets">1</span></dt>
<dd><p><a class="reference external" href="https://hal.inria.fr/hal-00911179v1/document">[1]</a> Oquab M., Bottou L., Laptev I., Sivic J. “Learning and transferring mid-level image representations using convolutional neural networks”. In CVPR, 2014.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">adapt.utils</span> <span class="kn">import</span> <span class="n">make_regression_da</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">adapt.parameter_based</span> <span class="kn">import</span> <span class="n">FineTuning</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="n">Xt</span><span class="p">,</span> <span class="n">yt</span> <span class="o">=</span> <span class="n">make_regression_da</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">src_model</span> <span class="o">=</span> <span class="n">FineTuning</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s2">&quot;mse&quot;</span><span class="p">,</span> <span class="n">pretrain</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">src_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">src_model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">Xt</span><span class="p">,</span> <span class="n">yt</span><span class="p">))</span>
<span class="go">1/1 [==============================] - 0s 82ms/step - loss: 0.2645</span>
<span class="go">0.26447802782058716</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tgt_model</span> <span class="o">=</span> <span class="n">FineTuning</span><span class="p">(</span><span class="n">encoder</span><span class="o">=</span><span class="n">src_model</span><span class="o">.</span><span class="n">encoder_</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s2">&quot;mse&quot;</span><span class="p">,</span> <span class="n">pretrain</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="gp">... </span>                       <span class="n">pretrain__epochs</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tgt_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Xt</span><span class="p">[:</span><span class="mi">3</span><span class="p">],</span> <span class="n">yt</span><span class="p">[:</span><span class="mi">3</span><span class="p">],</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tgt_model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">Xt</span><span class="p">,</span> <span class="n">yt</span><span class="p">)</span>
<span class="go">1/1 [==============================] - 0s 78ms/step - loss: 0.1114</span>
<span class="go">0.11144775152206421</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Attributes</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>encoder_</strong><span class="classifier">tensorflow Model</span></dt><dd><p>encoder network.</p>
</dd>
<dt><strong>task_</strong><span class="classifier">tensorflow Model</span></dt><dd><p>Network.</p>
</dd>
<dt><strong>history_</strong><span class="classifier">dict</span></dt><dd><p>history of the losses and metrics across the epochs
of the network training.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#adapt.parameter_based.FineTuning.__init__" title="adapt.parameter_based.FineTuning.__init__"><code class="xref py py-obj docutils literal notranslate"><span class="pre">__init__</span></code></a>([encoder, task, Xt, yt, training, ...])</p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#adapt.parameter_based.FineTuning.compile" title="adapt.parameter_based.FineTuning.compile"><code class="xref py py-obj docutils literal notranslate"><span class="pre">compile</span></code></a>([optimizer, loss, metrics, ...])</p></td>
<td><p>Configures the model for training.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#adapt.parameter_based.FineTuning.fit" title="adapt.parameter_based.FineTuning.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>([Xt, yt])</p></td>
<td><p>Fit FineTuning.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#adapt.parameter_based.FineTuning.get_params" title="adapt.parameter_based.FineTuning.get_params"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_params</span></code></a>([deep])</p></td>
<td><p>Get parameters for this estimator.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#adapt.parameter_based.FineTuning.load_weights" title="adapt.parameter_based.FineTuning.load_weights"><code class="xref py py-obj docutils literal notranslate"><span class="pre">load_weights</span></code></a>(filepath[, skip_mismatch, ...])</p></td>
<td><p>Loads all layer weights from a saved files.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#adapt.parameter_based.FineTuning.predict" title="adapt.parameter_based.FineTuning.predict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict</span></code></a>(x[, batch_size, verbose, steps, ...])</p></td>
<td><p>Generates output predictions for the input samples.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#adapt.parameter_based.FineTuning.predict_disc" title="adapt.parameter_based.FineTuning.predict_disc"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict_disc</span></code></a>(X)</p></td>
<td><p>Not used.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#adapt.parameter_based.FineTuning.predict_task" title="adapt.parameter_based.FineTuning.predict_task"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict_task</span></code></a>(X)</p></td>
<td><p>Return predictions of the task on the encoded features.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#adapt.parameter_based.FineTuning.pretrain_step" title="adapt.parameter_based.FineTuning.pretrain_step"><code class="xref py py-obj docutils literal notranslate"><span class="pre">pretrain_step</span></code></a>(data)</p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#adapt.parameter_based.FineTuning.save_weights" title="adapt.parameter_based.FineTuning.save_weights"><code class="xref py py-obj docutils literal notranslate"><span class="pre">save_weights</span></code></a>(filepath[, overwrite, ...])</p></td>
<td><p>Saves all layer weights.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#adapt.parameter_based.FineTuning.score" title="adapt.parameter_based.FineTuning.score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">score</span></code></a>(X, y[, sample_weight])</p></td>
<td><p>Return the evaluation of the model on X, y.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#adapt.parameter_based.FineTuning.set_params" title="adapt.parameter_based.FineTuning.set_params"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_params</span></code></a>(**params)</p></td>
<td><p>Set the parameters of this estimator.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#adapt.parameter_based.FineTuning.transform" title="adapt.parameter_based.FineTuning.transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">transform</span></code></a>(X)</p></td>
<td><p>Return the encoded features of X.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#adapt.parameter_based.FineTuning.unsupervised_score" title="adapt.parameter_based.FineTuning.unsupervised_score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">unsupervised_score</span></code></a>(Xs, Xt)</p></td>
<td><p>Return unsupervised score.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt class="sig sig-object py" id="adapt.parameter_based.FineTuning.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">encoder</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">task</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Xt</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">yt</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pretrain</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">copy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">params</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/antoinedemathelin/adapt/tree/master/adapt/parameter_based/_finetuning.__init__.py"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#adapt.parameter_based.FineTuning.__init__" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="adapt.parameter_based.FineTuning.compile">
<span class="sig-name descname"><span class="pre">compile</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metrics</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_weights</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weighted_metrics</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">run_eagerly</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">steps_per_execution</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/antoinedemathelin/adapt/tree/master/adapt/parameter_based/_finetuning.compile.py"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#adapt.parameter_based.FineTuning.compile" title="Permalink to this definition"></a></dt>
<dd><p>Configures the model for training.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>optimizer: str or `tf.keras.optimizer` instance</strong></dt><dd><p>Optimizer</p>
</dd>
<dt><strong>loss: str or `tf.keras.losses.Loss` instance</strong></dt><dd><p>Loss function. A loss function is any callable
with the signature <cite>loss = fn(y_true, y_pred)</cite>,
where <cite>y_true</cite> are the ground truth values, and
<cite>y_pred</cite> are the model’s predictions.
<cite>y_true</cite> should have shape
<cite>(batch_size, d0, .. dN)</cite> (except in the case of
sparse loss functions such as
sparse categorical crossentropy which expects integer arrays of shape
<cite>(batch_size, d0, .. dN-1)</cite>).
<cite>y_pred</cite> should have shape <cite>(batch_size, d0, .. dN)</cite>.
The loss function should return a float tensor.
If a custom <cite>Loss</cite> instance is
used and reduction is set to <cite>None</cite>, return value has shape
<cite>(batch_size, d0, .. dN-1)</cite> i.e. per-sample or per-timestep loss
values; otherwise, it is a scalar. If the model has multiple outputs,
you can use a different loss on each output by passing a dictionary
or a list of losses. The loss value that will be minimized by the
model will then be the sum of all individual losses, unless
<cite>loss_weights</cite> is specified.</p>
</dd>
<dt><strong>metrics: list of str or list of `tf.keras.metrics.Metric` instance</strong></dt><dd><p>List of metrics to be evaluated by the model during training
and testing. Typically you will use <cite>metrics=[‘accuracy’]</cite>. A
function is any callable with the signature <cite>result = fn(y_true,
y_pred)</cite>. To specify different metrics for different outputs of a
multi-output model, you could also pass a dictionary, such as
<cite>metrics={‘output_a’: ‘accuracy’, ‘output_b’: [‘accuracy’, ‘mse’]}</cite>.
You can also pass a list to specify a metric or a list of metrics
for each output, such as <cite>metrics=[[‘accuracy’], [‘accuracy’, ‘mse’]]</cite>
or <cite>metrics=[‘accuracy’, [‘accuracy’, ‘mse’]]</cite>. When you pass the
strings ‘accuracy’ or ‘acc’, we convert this to one of
<cite>tf.keras.metrics.BinaryAccuracy</cite>,
<cite>tf.keras.metrics.CategoricalAccuracy</cite>,
<cite>tf.keras.metrics.SparseCategoricalAccuracy</cite> based on the loss
function used and the model output shape. We do a similar
conversion for the strings ‘crossentropy’ and ‘ce’ as well.</p>
</dd>
<dt><strong>loss_weights: List or dict of floats</strong></dt><dd><p>Scalars to weight the loss contributions of different model
outputs. The loss value that will be minimized by the model will then
be the <em>weighted sum</em> of all individual losses, weighted by the
<cite>loss_weights</cite> coefficients.
If a list, it is expected to have a 1:1 mapping to the model’s
outputs. If a dict, it is expected to map output names (strings)
to scalar coefficients.</p>
</dd>
<dt><strong>weighted_metrics: list of metrics</strong></dt><dd><p>List of metrics to be evaluated and weighted by
<cite>sample_weight</cite> or <cite>class_weight</cite> during training and testing.</p>
</dd>
<dt><strong>run_eagerly: bool (default=False)</strong></dt><dd><p>If <cite>True</cite>, this <cite>Model</cite>’s logic will not be wrapped
in a <cite>tf.function</cite>. Recommended to leave
this as <cite>None</cite> unless your <cite>Model</cite> cannot be run inside a
<cite>tf.function</cite>. <cite>run_eagerly=True</cite> is not supported when using
<cite>tf.distribute.experimental.ParameterServerStrategy</cite>.</p>
</dd>
<dt><strong>steps_per_execution: int (default=1)</strong></dt><dd><p>The number of batches to run during each
<cite>tf.function</cite> call. Running multiple batches
inside a single <cite>tf.function</cite> call can greatly improve performance
on TPUs or small models with a large Python overhead.
At most, one full epoch will be run each
execution. If a number larger than the size of the epoch is passed,
the execution will be truncated to the size of the epoch.
Note that if <cite>steps_per_execution</cite> is set to <cite>N</cite>,
<cite>Callback.on_batch_begin</cite> and <cite>Callback.on_batch_end</cite> methods
will only be called every <cite>N</cite> batches
(i.e. before/after each <cite>tf.function</cite> execution).</p>
</dd>
<dt><strong>**kwargs: key, value arguments</strong></dt><dd><p>Arguments supported for backwards compatibility only.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>None: None</dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="adapt.parameter_based.FineTuning.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">Xt</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">yt</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">fit_params</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/antoinedemathelin/adapt/tree/master/adapt/parameter_based/_finetuning.fit.py"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#adapt.parameter_based.FineTuning.fit" title="Permalink to this definition"></a></dt>
<dd><p>Fit FineTuning.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>Xt</strong><span class="classifier">numpy array (default=None)</span></dt><dd><p>Target input data.</p>
</dd>
<dt><strong>yt</strong><span class="classifier">numpy array (default=None)</span></dt><dd><p>Target output data.</p>
</dd>
<dt><strong>fit_params</strong><span class="classifier">key, value arguments</span></dt><dd><p>Arguments given to the fit method of the model
(epochs, batch_size, callbacks…).</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>self</strong><span class="classifier">returns an instance of self</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="adapt.parameter_based.FineTuning.get_params">
<span class="sig-name descname"><span class="pre">get_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">deep</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/antoinedemathelin/adapt/tree/master/adapt/parameter_based/_finetuning.get_params.py"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#adapt.parameter_based.FineTuning.get_params" title="Permalink to this definition"></a></dt>
<dd><p>Get parameters for this estimator.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>deep</strong><span class="classifier">bool, default=True</span></dt><dd><p>Not used, here for scikit-learn compatibility.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>params</strong><span class="classifier">dict</span></dt><dd><p>Parameter names mapped to their values.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="adapt.parameter_based.FineTuning.load_weights">
<span class="sig-name descname"><span class="pre">load_weights</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">filepath</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">skip_mismatch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">by_name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">options</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/antoinedemathelin/adapt/tree/master/adapt/parameter_based/_finetuning.load_weights.py"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#adapt.parameter_based.FineTuning.load_weights" title="Permalink to this definition"></a></dt>
<dd><p>Loads all layer weights from a saved files.</p>
<p>The saved file could be a SavedModel file, a <cite>.keras</cite> file (v3 saving
format), or a file created via <cite>model.save_weights()</cite>.</p>
<p>By default, weights are loaded based on the network’s
topology. This means the architecture should be the same as when the
weights were saved. Note that layers that don’t have weights are not
taken into account in the topological ordering, so adding or removing
layers is fine as long as they don’t have weights.</p>
<p><strong>Partial weight loading</strong></p>
<p>If you have modified your model, for instance by adding a new layer
(with weights) or by changing the shape of the weights of a layer,
you can choose to ignore errors and continue loading
by setting <cite>skip_mismatch=True</cite>. In this case any layer with
mismatching weights will be skipped. A warning will be displayed
for each skipped layer.</p>
<p><strong>Weight loading by name</strong></p>
<p>If your weights are saved as a <cite>.h5</cite> file created
via <cite>model.save_weights()</cite>, you can use the argument <cite>by_name=True</cite>.</p>
<p>In this case, weights are loaded into layers only if they share
the same name. This is useful for fine-tuning or transfer-learning
models where some of the layers have changed.</p>
<p>Note that only topological loading (<cite>by_name=False</cite>) is supported when
loading weights from the <cite>.keras</cite> v3 format or from the TensorFlow
SavedModel format.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>filepath: String, path to the weights file to load. For weight files</dt><dd><p>in TensorFlow format, this is the file prefix (the same as was
passed to <cite>save_weights()</cite>). This can also be a path to a
SavedModel or a <cite>.keras</cite> file (v3 saving format) saved
via <cite>model.save()</cite>.</p>
</dd>
<dt>skip_mismatch: Boolean, whether to skip loading of layers where</dt><dd><p>there is a mismatch in the number of weights, or a mismatch in
the shape of the weights.</p>
</dd>
<dt>by_name: Boolean, whether to load weights by name or by topological</dt><dd><p>order. Only topological loading is supported for weight files in
the <cite>.keras</cite> v3 format or in the TensorFlow SavedModel format.</p>
</dd>
<dt>options: Optional <cite>tf.train.CheckpointOptions</cite> object that specifies</dt><dd><p>options for loading weights (only valid for a SavedModel file).</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="adapt.parameter_based.FineTuning.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">steps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">callbacks</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_queue_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">workers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_multiprocessing</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/antoinedemathelin/adapt/tree/master/adapt/parameter_based/_finetuning.predict.py"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#adapt.parameter_based.FineTuning.predict" title="Permalink to this definition"></a></dt>
<dd><p>Generates output predictions for the input samples.</p>
<p>Computation is done in batches. This method is designed for performance in
large scale inputs. For small amount of inputs that fit in one batch,
directly using <cite>__call__()</cite> is recommended for faster execution, e.g.,
<cite>model(x)</cite>, or <cite>model(x, training=False)</cite> if you have layers such as
<cite>tf.keras.layers.BatchNormalization</cite> that behaves differently during
inference. Also, note the fact that test loss is not affected by
regularization layers like noise and dropout.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>x: array</strong></dt><dd><p>Input samples.</p>
</dd>
<dt><strong>batch_size: int (default=`None`)</strong></dt><dd><p>Number of samples per batch.
If unspecified, <cite>batch_size</cite> will default to 32.
Do not specify the <cite>batch_size</cite> if your data is in the
form of dataset, generators, or <cite>keras.utils.Sequence</cite> instances
(since they generate batches).</p>
</dd>
<dt><strong>verbose: int (default=0)</strong></dt><dd><p>Verbosity mode, 0 or 1.</p>
</dd>
<dt><strong>steps: int (default=None)</strong></dt><dd><p>Total number of steps (batches of samples)
before declaring the prediction round finished.
Ignored with the default value of <cite>None</cite>. If x is a <cite>tf.data</cite>
dataset and <cite>steps</cite> is None, <cite>predict()</cite> will
run until the input dataset is exhausted.</p>
</dd>
<dt><strong>callbacks: List of `keras.callbacks.Callback` instances.</strong></dt><dd><p>List of callbacks to apply during prediction.
See [callbacks](/api_docs/python/tf/keras/callbacks).</p>
</dd>
<dt><strong>max_queue_size: int (default=10)</strong></dt><dd><p>Used for generator or <cite>keras.utils.Sequence</cite>
input only. Maximum size for the generator queue.
If unspecified, <cite>max_queue_size</cite> will default to 10.</p>
</dd>
<dt><strong>workers: int (default=1)</strong></dt><dd><p>Used for generator or <cite>keras.utils.Sequence</cite> input
only. Maximum number of processes to spin up when using
process-based threading. If unspecified, <cite>workers</cite> will default
to 1.</p>
</dd>
<dt><strong>use_multiprocessing: bool (default=False)</strong></dt><dd><p>Used for generator or <cite>keras.utils.Sequence</cite> input only.
If <cite>True</cite>, use process-based
threading. If unspecified, <cite>use_multiprocessing</cite> will default to
<cite>False</cite>. Note that because this implementation relies on
multiprocessing, you should not pass non-picklable arguments to
the generator as they can’t be passed easily to children processes.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>y_pred</strong><span class="classifier">array</span></dt><dd><p>Numpy array(s) of predictions.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="adapt.parameter_based.FineTuning.predict_disc">
<span class="sig-name descname"><span class="pre">predict_disc</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/antoinedemathelin/adapt/tree/master/adapt/parameter_based/_finetuning.predict_disc.py"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#adapt.parameter_based.FineTuning.predict_disc" title="Permalink to this definition"></a></dt>
<dd><p>Not used.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="adapt.parameter_based.FineTuning.predict_task">
<span class="sig-name descname"><span class="pre">predict_task</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/antoinedemathelin/adapt/tree/master/adapt/parameter_based/_finetuning.predict_task.py"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#adapt.parameter_based.FineTuning.predict_task" title="Permalink to this definition"></a></dt>
<dd><p>Return predictions of the task on the encoded features.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array</span></dt><dd><p>input data</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>y_task</strong><span class="classifier">array</span></dt><dd><p>predictions of task network</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="adapt.parameter_based.FineTuning.pretrain_step">
<span class="sig-name descname"><span class="pre">pretrain_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/antoinedemathelin/adapt/tree/master/adapt/parameter_based/_finetuning.pretrain_step.py"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#adapt.parameter_based.FineTuning.pretrain_step" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="adapt.parameter_based.FineTuning.save_weights">
<span class="sig-name descname"><span class="pre">save_weights</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">filepath</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">overwrite</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">save_format</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">options</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/antoinedemathelin/adapt/tree/master/adapt/parameter_based/_finetuning.save_weights.py"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#adapt.parameter_based.FineTuning.save_weights" title="Permalink to this definition"></a></dt>
<dd><p>Saves all layer weights.</p>
<p>Either saves in HDF5 or in TensorFlow format based on the <cite>save_format</cite>
argument.</p>
<dl class="simple">
<dt>When saving in HDF5 format, the weight file has:</dt><dd><ul class="simple">
<li><dl class="simple">
<dt><cite>layer_names</cite> (attribute), a list of strings</dt><dd><p>(ordered names of model layers).</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>For every layer, a <cite>group</cite> named <cite>layer.name</cite></dt><dd><ul>
<li><dl class="simple">
<dt>For every such layer group, a group attribute <cite>weight_names</cite>,</dt><dd><p>a list of strings
(ordered names of weights tensor of the layer).</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>For every weight in the layer, a dataset</dt><dd><p>storing the weight value, named after the weight tensor.</p>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
<p>When saving in TensorFlow format, all objects referenced by the network
are saved in the same format as <cite>tf.train.Checkpoint</cite>, including any
<cite>Layer</cite> instances or <cite>Optimizer</cite> instances assigned to object
attributes. For networks constructed from inputs and outputs using
<cite>tf.keras.Model(inputs, outputs)</cite>, <cite>Layer</cite> instances used by the network
are tracked/saved automatically. For user-defined classes which inherit
from <cite>tf.keras.Model</cite>, <cite>Layer</cite> instances must be assigned to object
attributes, typically in the constructor. See the documentation of
<cite>tf.train.Checkpoint</cite> and <cite>tf.keras.Model</cite> for details.</p>
<p>While the formats are the same, do not mix <cite>save_weights</cite> and
<cite>tf.train.Checkpoint</cite>. Checkpoints saved by <cite>Model.save_weights</cite> should
be loaded using <cite>Model.load_weights</cite>. Checkpoints saved using
<cite>tf.train.Checkpoint.save</cite> should be restored using the corresponding
<cite>tf.train.Checkpoint.restore</cite>. Prefer <cite>tf.train.Checkpoint</cite> over
<cite>save_weights</cite> for training checkpoints.</p>
<p>The TensorFlow format matches objects and variables by starting at a
root object, <cite>self</cite> for <cite>save_weights</cite>, and greedily matching attribute
names. For <cite>Model.save</cite> this is the <cite>Model</cite>, and for <cite>Checkpoint.save</cite>
this is the <cite>Checkpoint</cite> even if the <cite>Checkpoint</cite> has a model attached.
This means saving a <cite>tf.keras.Model</cite> using <cite>save_weights</cite> and loading
into a <cite>tf.train.Checkpoint</cite> with a <cite>Model</cite> attached (or vice versa)
will not match the <cite>Model</cite>’s variables. See the
[guide to training checkpoints](
<a class="reference external" href="https://www.tensorflow.org/guide/checkpoint">https://www.tensorflow.org/guide/checkpoint</a>) for details on
the TensorFlow format.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>filepath: String or PathLike, path to the file to save the weights</dt><dd><p>to. When saving in TensorFlow format, this is the prefix used
for checkpoint files (multiple files are generated). Note that
the ‘.h5’ suffix causes weights to be saved in HDF5 format.</p>
</dd>
<dt>overwrite: Whether to silently overwrite any existing file at the</dt><dd><p>target location, or provide the user with a manual prompt.</p>
</dd>
<dt>save_format: Either ‘tf’ or ‘h5’. A <cite>filepath</cite> ending in ‘.h5’ or</dt><dd><p>‘.keras’ will default to HDF5 if <cite>save_format</cite> is <cite>None</cite>.
Otherwise, <cite>None</cite> becomes ‘tf’. Defaults to <cite>None</cite>.</p>
</dd>
<dt>options: Optional <cite>tf.train.CheckpointOptions</cite> object that specifies</dt><dd><p>options for saving weights.</p>
</dd>
</dl>
</dd>
<dt>Raises:</dt><dd><dl class="simple">
<dt>ImportError: If <cite>h5py</cite> is not available when attempting to save in</dt><dd><p>HDF5 format.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="adapt.parameter_based.FineTuning.score">
<span class="sig-name descname"><span class="pre">score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/antoinedemathelin/adapt/tree/master/adapt/parameter_based/_finetuning.score.py"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#adapt.parameter_based.FineTuning.score" title="Permalink to this definition"></a></dt>
<dd><p>Return the evaluation of the model on X, y.</p>
<p>Call <cite>evaluate</cite> on tensorflow Model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array</span></dt><dd><p>input data</p>
</dd>
<dt><strong>y</strong><span class="classifier">array</span></dt><dd><p>output data</p>
</dd>
<dt><strong>sample_weight</strong><span class="classifier">array (default=None)</span></dt><dd><p>Sample weights</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>score</strong><span class="classifier">float</span></dt><dd><p>Score.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="adapt.parameter_based.FineTuning.set_params">
<span class="sig-name descname"><span class="pre">set_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">params</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/antoinedemathelin/adapt/tree/master/adapt/parameter_based/_finetuning.set_params.py"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#adapt.parameter_based.FineTuning.set_params" title="Permalink to this definition"></a></dt>
<dd><p>Set the parameters of this estimator.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>**params</strong><span class="classifier">dict</span></dt><dd><p>Estimator parameters.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>self</strong><span class="classifier">estimator instance</span></dt><dd><p>Estimator instance.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="adapt.parameter_based.FineTuning.transform">
<span class="sig-name descname"><span class="pre">transform</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/antoinedemathelin/adapt/tree/master/adapt/parameter_based/_finetuning.transform.py"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#adapt.parameter_based.FineTuning.transform" title="Permalink to this definition"></a></dt>
<dd><p>Return the encoded features of X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array</span></dt><dd><p>input data</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>X_enc</strong><span class="classifier">array</span></dt><dd><p>predictions of encoder network</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="adapt.parameter_based.FineTuning.unsupervised_score">
<span class="sig-name descname"><span class="pre">unsupervised_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">Xs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Xt</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/antoinedemathelin/adapt/tree/master/adapt/parameter_based/_finetuning.unsupervised_score.py"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#adapt.parameter_based.FineTuning.unsupervised_score" title="Permalink to this definition"></a></dt>
<dd><p>Return unsupervised score.</p>
<p>The normalized discrepancy distance is computed
between the reweighted/transformed source input
data and the target input data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>Xs</strong><span class="classifier">array</span></dt><dd><p>Source input data.</p>
</dd>
<dt><strong>Xt</strong><span class="classifier">array</span></dt><dd><p>Source input data.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>score</strong><span class="classifier">float</span></dt><dd><p>Unsupervised score.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<h2> Examples </h2><div class="toctree-wrapper compound">
</div>
<div class="sphx-glr-thumbcontainer">
  <div class="figure align-center">
    <img alt="thumbnail" src="../_static/images/finetuned.png" />
    <p class="caption">
      <span class="caption-text">
        <a class="reference internal" href="../examples/Flowers_example.html">
          <span class="std std-ref">Model-Based Transfer Learning</span>
        </a>
      </span>
    </p>
  </div>
</div>
<div class="sphx-glr-thumbcontainer">
  <div class="figure align-center">
    <img alt="thumbnail" src="../_static/images/office_item.png" />
    <p class="caption">
      <span class="caption-text">
        <a class="reference internal" href="../examples/Office_example.html">
          <span class="std std-ref">Deep Domain Adaptation: Transfer one domain to another</span>
        </a>
      </span>
    </p>
  </div>
</div>
<div class="sphx-glr-clear"></div></section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, ADAPT team, Michelin and Centre Borelli, ENS Paris-Saclay.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>